{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Columns: Index(['file', 'age', 'gender', 'race', 'service_test'], dtype='object')\n",
      "Validation Data Columns: Index(['file', 'age', 'gender', 'race', 'service_test'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load train and validation datasets separately\n",
    "train_df = pd.read_csv(r\"C:\\Users\\adamf\\Desktop\\race classifier\\fairface_label_train.csv\")\n",
    "val_df = pd.read_csv(r\"C:\\Users\\adamf\\Desktop\\race classifier\\fairface_label_val.csv\")\n",
    "\n",
    "print(\"Train Data Columns:\", train_df.columns)  # Check if 'file' and 'race_encoded' exist\n",
    "print(\"Validation Data Columns:\", val_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race Mapping: {'Black': 0, 'East Asian': 1, 'Indian': 2, 'Latino_Hispanic': 3, 'Middle Eastern': 4, 'Southeast Asian': 5, 'White': 6}\n"
     ]
    }
   ],
   "source": [
    "# Convert race labels to numbers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode race labels as numerical values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['race_encoded'] = label_encoder.fit_transform(train_df['race'])\n",
    "val_df['race_encoded'] = label_encoder.transform(val_df['race'])  # Use same encoding\n",
    "\n",
    "\n",
    "# Mapping of race categories\n",
    "# zip() pairs each category with its corresponding number and dict() creates a dictionary from the pairs fir easy reference\n",
    "race_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Race Mapping:\", race_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample filenames from train_df:\n",
      "0    train/1.jpg\n",
      "1    train/2.jpg\n",
      "2    train/3.jpg\n",
      "3    train/4.jpg\n",
      "4    train/5.jpg\n",
      "Name: file, dtype: object\n",
      "\n",
      "Files in training image directory:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_IMG_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# List some actual image files from the directory\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFiles in training image directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mTRAIN_IMG_DIR\u001b[49m)[:\u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# List the first 5 files\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN_IMG_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the first few file names from the DataFrame\n",
    "print(\"Sample filenames from train_df:\")\n",
    "print(train_df['file'].head())\n",
    "\n",
    "# List some actual image files from the directory\n",
    "print(\"\\nFiles in training image directory:\")\n",
    "print(os.listdir(TRAIN_IMG_DIR)[:5])  # List the first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define paths to image directories\n",
    "TRAIN_IMG_DIR = r\"C:\\Users\\adamf\\Desktop\\race classifier\\fairface-img-margin025-trainval\\train\"\n",
    "VAL_IMG_DIR = r\"C:\\Users\\adamf\\Desktop\\race classifier\\fairface-img-margin025-trainval\\val\"\n",
    "\n",
    "\n",
    "# Function to load images in batches\n",
    "def image_generator(df, img_dir, batch_size=64):\n",
    "    while True:  # Infinite loop for batch generation\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch_df = df.iloc[i:i+batch_size]  # Get batch\n",
    "            images = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                filename = row['file'].replace(\"train/\", \"\").replace(\"val/\", \"\")\n",
    "                img_path = os.path.join(img_dir, filename)\n",
    "\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue  # Skip missing images\n",
    "\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "                img = img.astype('float32') / 255.0  # Normalize\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(row['race_encoded'])\n",
    "\n",
    "            yield np.array(images), np.array(labels)  # Return batch\n",
    "\n",
    "# Create generators for training and validation\n",
    "train_gen = image_generator(train_df, TRAIN_IMG_DIR, batch_size=32)\n",
    "val_gen = image_generator(val_df, VAL_IMG_DIR, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 224, 224, 3) (32,)\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch of images from the training generator\n",
    "X_batch, y_batch = next(train_gen)\n",
    "\n",
    "# Print shape of the batch to verify\n",
    "print(\"Batch Shape:\", X_batch.shape, y_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 634s 1s/step - loss: 1.8410 - accuracy: 0.2655 - val_loss: 1.7029 - val_accuracy: 0.3634\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 604s 1s/step - loss: 1.6676 - accuracy: 0.3520 - val_loss: 1.5772 - val_accuracy: 0.3831\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 611s 1s/step - loss: 1.6175 - accuracy: 0.3681 - val_loss: 1.6118 - val_accuracy: 0.3744\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 610s 1s/step - loss: 1.5780 - accuracy: 0.3814 - val_loss: 1.4651 - val_accuracy: 0.4279\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 606s 1s/step - loss: 1.5436 - accuracy: 0.4008 - val_loss: 1.4648 - val_accuracy: 0.4263\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 612s 1s/step - loss: 1.5096 - accuracy: 0.4164 - val_loss: 1.4461 - val_accuracy: 0.4428\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 615s 1s/step - loss: 1.4835 - accuracy: 0.4219 - val_loss: 1.4182 - val_accuracy: 0.4443\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 604s 1s/step - loss: 1.4635 - accuracy: 0.4352 - val_loss: 1.3933 - val_accuracy: 0.4747\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 647s 1s/step - loss: 1.4484 - accuracy: 0.4383 - val_loss: 1.3771 - val_accuracy: 0.4688\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 610s 1s/step - loss: 1.4279 - accuracy: 0.4468 - val_loss: 1.3963 - val_accuracy: 0.4569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2762865ad70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN ARCHITECTURE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    # First Convolutional Block\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Prevent overfitting\n",
    "    Dense(7, activation='softmax')  # 7 classes (one per race category)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Use below if your PC is fast\n",
    "# model.fit(\n",
    "#     train_gen, \n",
    "#     validation_data=val_gen, \n",
    "#     steps_per_epoch=len(train_df) // 64,  # Batches per epoch\n",
    "#     validation_steps=len(val_df) // 64, \n",
    "#     epochs=10\n",
    "# )\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch=500,  # Lower total steps per epoch\n",
    "    validation_steps=100,  # Lower validation steps\n",
    "    epochs=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"race_classifier_model_v1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"race_classifier_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
